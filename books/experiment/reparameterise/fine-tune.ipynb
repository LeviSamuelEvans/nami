{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e96604d",
   "metadata": {},
   "source": [
    "## fine-tuning a flow model with a physics constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078da5d1",
   "metadata": {},
   "source": [
    "train a basic flow matching model on 2D Gaussian data, then fine-tune it so generated samples have a specific mean. In HEP, some simulator that generates events given some theory parameters theta, but no explicit likelihood p(d | theta), train fm model as conditional density estimator for posterior p(theta | obs data), then use rsample to fine-tune flow. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2a2366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1: standard flow matching training\n",
      "  Step 100, FM loss: 1.5353\n",
      "  Step 200, FM loss: 0.9963\n",
      "  Step 300, FM loss: 0.9572\n",
      "  Step 400, FM loss: 1.0478\n",
      "  Step 500, FM loss: 0.9572\n",
      "\n",
      "Before fine-tuning: sample mean = [3.038644313812256, 3.0558784008026123]\n",
      "\n",
      "P2: Fine-tuning (via rsample)\n",
      "  Step 50, physics loss: 0.0030, sample mean: [4.858842849731445, 4.845598220825195]\n",
      "  Step 100, physics loss: 0.0294, sample mean: [5.056225299835205, 5.076822757720947]\n",
      "  Step 150, physics loss: 0.0496, sample mean: [4.948419570922852, 4.893041610717773]\n",
      "  Step 200, physics loss: 0.0748, sample mean: [5.01827335357666, 5.036735534667969]\n",
      "\n",
      "After fine-tuning: sample mean = [5.059838771820068, 5.076112747192383]\n",
      "Target mean:                     [5.0, 5.0]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import nami\n",
    "\n",
    "class ToyField(nn.Module):\n",
    "    def __init__(self, dim=2, hidden=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim + 1, hidden), nn.SiLU(),\n",
    "            nn.Linear(hidden, hidden), nn.SiLU(),\n",
    "            nn.Linear(hidden, dim),\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def event_ndim(self):\n",
    "        return 1\n",
    "\n",
    "    def forward(self, x, t, c=None):\n",
    "        t_exp = t.unsqueeze(-1).expand(*x.shape[:-1], 1)\n",
    "        return self.net(torch.cat([x, t_exp], dim=-1))\n",
    "\n",
    "dim = 2\n",
    "field = ToyField(dim)\n",
    "base = nami.StandardNormal(event_shape=(dim,))\n",
    "solver = nami.RK4(steps=32)\n",
    "optimizer = torch.optim.Adam(field.parameters(), lr=1e-3)\n",
    "\n",
    "# 2D Gaussian centered at (3, 3)\n",
    "data_mean = torch.tensor([3.0, 3.0])\n",
    "\n",
    "print(\"P1: standard flow matching training\")\n",
    "for step in range(500):\n",
    "    x_target = data_mean + 0.5 * torch.randn(256, dim)\n",
    "    x_source = torch.randn_like(x_target)\n",
    "    loss = nami.fm_loss(field, x_target, x_source)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (step + 1) % 100 == 0:\n",
    "        print(f\"  Step {step+1}, FM loss: {loss.item():.4f}\")\n",
    "\n",
    "fm = nami.FlowMatching(field, base, solver, event_ndim=1)\n",
    "process = fm(None)\n",
    "with torch.no_grad():\n",
    "    pre_samples = process.sample((500,))\n",
    "print(f\"\\nBefore fine-tuning: sample mean = {pre_samples.mean(0).tolist()}\")\n",
    "\n",
    "# fine-tune\n",
    "target_mean = torch.tensor([5.0, 5.0])\n",
    "ft_optimiser = torch.optim.Adam(field.parameters(), lr=5e-4)\n",
    "\n",
    "print(\"\\nP2: Fine-tuning (via rsample)\")\n",
    "for step in range(200):\n",
    "    process = fm(None)\n",
    "    \n",
    "    # rsample lets gradients flow back into field\n",
    "    samples = process.rsample((64,))\n",
    "    # push the generated distributions mean toward target\n",
    "    physics_loss = (samples.mean(0) - target_mean).pow(2).sum()\n",
    "    \n",
    "    ft_optimiser.zero_grad()\n",
    "    physics_loss.backward()\n",
    "    ft_optimiser.step()\n",
    "    \n",
    "    if (step + 1) % 50 == 0:\n",
    "        with torch.no_grad():\n",
    "            check = process.sample((500,))\n",
    "        print(f\"  Step {step+1}, physics loss: {physics_loss.item():.4f}, \"\n",
    "              f\"sample mean: {check.mean(0).tolist()}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    post_samples = process.sample((500,))\n",
    "print(f\"\\nAfter fine-tuning: sample mean = {post_samples.mean(0).tolist()}\")\n",
    "print(f\"Target mean:                     {target_mean.tolist()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nami",
   "language": "python",
   "name": "nami"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
