from __future__ import annotations

import torch

from ..core.specs import split_event
from .base import DivergenceEstimator


def _rademacher_like(x: torch.Tensor) -> torch.Tensor:
    return torch.empty_like(x).bernoulli_(0.5).mul_(2).sub_(1)


class HutchinsonDivergence(DivergenceEstimator):
    def __init__(self, probe: str = "rademacher"):
        if probe not in {"rademacher", "gaussian"}:
            msg = "probe must be 'rademacher' or 'gaussian'"
            raise ValueError(msg)
        self.probe = probe

    def __call__(
        self, field, x: torch.Tensor, t: torch.Tensor, c: torch.Tensor | None
    ) -> torch.Tensor:
        event_ndim = getattr(field, "event_ndim", None)
        if event_ndim is None:
            msg = "field.event_ndim is required for divergence"
            raise ValueError(msg)
        lead, _event_shape = split_event(x, event_ndim)

        with torch.enable_grad():
            # clone to avoid mutating input tensor's grad state
            x_req = x.detach().clone().requires_grad_(True)

            # field call must be inside enable_grad() to build computation graph
            v = field(x_req, t, c)

            if self.probe == "gaussian":
                eps = torch.randn_like(x_req)
            else:
                eps = _rademacher_like(x_req)

            dot = (v * eps).sum()
            grad = torch.autograd.grad(dot, x_req, create_graph=False)[0]

        return (grad * eps).reshape(*lead, -1).sum(dim=-1)
